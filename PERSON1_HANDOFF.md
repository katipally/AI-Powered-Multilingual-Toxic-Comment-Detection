# Person 1 â†’ Person 2: Handoff Document

**Date:** October 30, 2025  
**From:** Person 1 - Data Collection & Preprocessing Lead  
**To:** Person 2 - Annotation Lead  
**Status:** âœ… **COMPLETE** - All Person 1 deliverables ready

---

## ğŸ¯ Executive Summary

Person 1 tasks are **100% complete**. This package includes:

âœ… **218,675 total samples** collected and preprocessed  
âœ… **198,603 labeled samples** (70/15/15 train/dev/test splits)  
âœ… **20,072 unlabeled samples** ready for Person 2 annotation  
âœ… **Normalization & transliteration utilities** (validated â‰¥95% accuracy)  
âœ… **Deduplication utilities** (exact + near-duplicate detection)  
âœ… **Complete data card** with TOS compliance  
âœ… **Quality reports** (100% validation passed)  
âœ… **DVC-ready structure** for reproducibility  
âœ… **All code documented** and tested

**What Person 2 Needs to Do:**
1. Annotate 20,072 unlabeled samples (Reddit + YouTube)
2. Focus on 1,869 code-mixed samples (priority)
3. Calculate inter-annotator agreement
4. Merge annotations and create final dataset

---

## ğŸ“ Package Contents

```
Github_upload/
â”œâ”€â”€ PERSON1_HANDOFF.md          â† YOU ARE HERE
â”œâ”€â”€ README.md                    â† Quick start guide
â”œâ”€â”€ DATA_CARD.md                 â† Complete dataset documentation
â”œâ”€â”€ requirements.txt             â† All dependencies
â”œâ”€â”€ .gitignore                   â† Git ignore rules
â”‚
â”œâ”€â”€ data/                        â† All data files
â”‚   â”œâ”€â”€ splits/                  â† Train/dev/test splits
â”‚   â”‚   â”œâ”€â”€ train.csv           (139,022 samples)
â”‚   â”‚   â”œâ”€â”€ dev.csv             (29,790 samples)
â”‚   â”‚   â”œâ”€â”€ test.csv            (29,791 samples)
â”‚   â”‚   â””â”€â”€ split_manifest.json
â”‚   â”‚
â”‚   â”œâ”€â”€ labeled/                 â† Pre-labeled data
â”‚   â”‚   â”œâ”€â”€ all_labeled_data.csv (198,603 samples)
â”‚   â”‚   â”œâ”€â”€ hatexplain.csv
â”‚   â”‚   â”œâ”€â”€ jigsaw_bias.csv
â”‚   â”‚   â”œâ”€â”€ jigsaw_multilingual.csv
â”‚   â”‚   â””â”€â”€ textdetox.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ unlabeled/               â† FOR PERSON 2 ANNOTATION
â”‚   â”‚   â”œâ”€â”€ for_annotation.csv  (20,072 samples) â­
â”‚   â”‚   â”œâ”€â”€ reddit.csv          (10,000 samples)
â”‚   â”‚   â””â”€â”€ youtube.csv         (10,072 samples)
â”‚   â”‚
â”‚   â””â”€â”€ reports/                 â† Quality & statistics
â”‚       â”œâ”€â”€ processing_report.json
â”‚       â””â”€â”€ validation_report.json
â”‚
â”œâ”€â”€ scripts/                     â† All collection & preprocessing scripts
â”‚   â”œâ”€â”€ 1_download_hatexplain.py
â”‚   â”œâ”€â”€ 2_collect_reddit.py
â”‚   â”œâ”€â”€ 3_collect_youtube.py
â”‚   â”œâ”€â”€ 4_download_textdetox.py
â”‚   â”œâ”€â”€ 5_preprocess_and_unify.py
â”‚   â”œâ”€â”€ 6_data_quality_checks.py
â”‚   â”œâ”€â”€ 7_create_stratified_splits.py
â”‚   â””â”€â”€ find_youtube_videos.py
â”‚
â”œâ”€â”€ utils/                       â† Normalization & deduplication
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ text_normalization.py   â† Romanized Hindi normalization
â”‚   â””â”€â”€ deduplication.py         â† Exact + near-duplicate detection
â”‚
â”œâ”€â”€ notebooks/                   â† Jupyter notebooks
â”‚   â””â”€â”€ collection_walkthrough.ipynb
â”‚
â””â”€â”€ docs/                        â† Additional documentation
    â”œâ”€â”€ COLLECTION_METHODOLOGY.md
    â”œâ”€â”€ PREPROCESSING_GUIDE.md
    â””â”€â”€ TOS_COMPLIANCE.md
```

---

## âœ… Person 1 Checklist: Completed Deliverables

### Required Outputs (from official requirements)

- [x] **Curated dataset with train/dev/test splits**
  - âœ… 70/15/15 stratified splits
  - âœ… Balanced by label + language + code-mixed status
  - âœ… Random seed 42 (frozen for reproducibility)
  - âœ… Split manifest with complete metadata
  
- [x] **Normalization & transliteration utilities**
  - âœ… Romanized Hindi word normalization (50+ words)
  - âœ… URL/email/mention removal
  - âœ… Punctuation & whitespace normalization
  - âœ… Validated on gold list (â‰¥95% accuracy)
  - âœ… Multiple presets (default, strict, code_mixed)
  
- [x] **Provenance logs, data card**
  - âœ… Complete data card with all 11 sections
  - âœ… License & TOS compliance documented
  - âœ… Collection queries & sampling recorded
  - âœ… Source attribution for all datasets
  
- [x] **Quality report**
  - âœ… Dedup rate: 1.47% (labeled), 14.04% (unlabeled)
  - âœ… Language mix: 15+ languages documented
  - âœ… Class balance: 72.3% non-toxic, 27.7% toxic
  - âœ… PII scrub: URLs, emails, mentions removed
  
- [x] **DVC-tracked structure**
  - âœ… DVC configuration ready
  - âœ… Reproducible from raw to processed
  - âœ… No credential leakage (.env excluded)

### Detailed Responsibilities

- [x] **Implement collectors with official SDKs/APIs**
  - âœ… PRAW for Reddit (rate-limited to 100 QPM)
  - âœ… Google API for YouTube (quota-safe)
  - âœ… HuggingFace datasets for public corpora
  - âœ… Raw JSONL saved with query params
  
- [x] **Code-mix detection pipeline**
  - âœ… Script ratio detection (Latin vs Devanagari)
  - âœ… Token-level language ID (langdetect)
  - âœ… Pattern matching for Romanized Hindi
  - âœ… 1,869 code-mixed samples identified
  
- [x] **Cleaning pipeline**
  - âœ… Strip URLs/HTML (replaced with tokens)
  - âœ… Normalize punctuation/emoji
  - âœ… Transliteration normalization (Romanized Hindi)
  - âœ… Unicode normalization (NFKC)
  
- [x] **Deduplicate**
  - âœ… By normalized text hash (MD5)
  - âœ… Near-duplicate detection available (cosine sim)
  - âœ… Dedup rates documented in quality report
  
- [x] **Stratified splits**
  - âœ… By toxicity label AND language mix
  - âœ… Frozen random seed (42)
  - âœ… Split manifest stored
  
- [x] **Data card + collection notebook**
  - âœ… 11-section data card (5,000+ words)
  - âœ… Collection notebook with examples
  - âœ… All artifacts DVC-ready

---

## ğŸ“Š Dataset Statistics for Person 2

### What You're Receiving

| Dataset | Samples | Purpose |
|---------|---------|---------|
| **Labeled (ready to use)** | 198,603 | Train/dev/test your models |
| **Unlabeled (your work)** | 20,072 | Annotate as toxic (1) or non-toxic (0) |
| **Code-mixed (priority)** | 1,869 | Focus annotation here first |

### Label Distribution (Labeled Data)

- Non-toxic (0): 143,536 (72.3%)
- Toxic (1): 55,067 (27.7%)
- **Target for unlabeled:** Maintain similar distribution

### Splits (Labeled Data)

| Split | Samples | Toxic % | Use |
|-------|---------|---------|-----|
| Train | 139,022 | 27.7% | Model training |
| Dev | 29,790 | 27.7% | Hyperparameter tuning |
| Test | 29,791 | 27.7% | Final evaluation |

### Unlabeled Breakdown

| Source | Total | Code-Mixed | Notes |
|--------|-------|------------|-------|
| Reddit | 10,000 | 927 (9.3%) | Indian subreddits |
| YouTube | 10,072 | 942 (9.4%) | Bollywood videos |

---

## ğŸš€ Quick Start for Person 2

### 1. Setup Environment

```bash
# Clone repository
git clone [repo_url]
cd toxic-comment-detection

# Install dependencies
pip install -r requirements.txt

# Verify data
python scripts/6_data_quality_checks.py
```

### 2. Load Data for Annotation

```python
import pandas as pd

# Load unlabeled data
df = pd.read_csv('data/unlabeled/for_annotation.csv')

# Priority: code-mixed samples
code_mixed = df[df['code_mixed'] == True]
print(f"Code-mixed samples to annotate: {len(code_mixed)}")

# Filter by source if needed
reddit = df[df['source'] == 'reddit']
youtube = df[df['source'] == 'youtube']
```

### 3. Annotation Schema

```python
# Each sample needs:
{
    'id': '[existing id]',
    'text': '[existing text]',
    'label': 0 or 1,  # YOUR ANNOTATION
    'annotator_id': '[your id]',
    'confidence': 'low'|'medium'|'high',
    'toxic_types': ['hate', 'threat', 'insult'],  # if toxic
    'notes': '[optional notes]'
}
```

### 4. Use Normalization Utilities

```python
from utils.text_normalization import normalize_text, get_normalizer

# Normalize text before showing to annotators
normalizer = get_normalizer('code_mixed')
cleaned_text = normalizer(raw_text)

# Or use full pipeline
normalized = normalize_text(
    text,
    normalize_hindi=True,
    remove_url=True,
    keep_emoji=True
)
```

---

## ğŸ¯ Person 2 Action Items

### Immediate (Week 1)

1. **Review this handoff document** âœ“
2. **Install environment** and verify data loads
3. **Read DATA_CARD.md** for dataset details
4. **Explore `data/unlabeled/for_annotation.csv`**
5. **Create annotation guidelines** for your team
6. **Set up annotation interface** (Label Studio, Prodigy, etc.)

### Annotation Phase (Weeks 2-4)

7. **Annotate code-mixed samples first** (1,869 samples - priority!)
8. **Annotate remaining samples** (18,203 samples)
9. **Calculate inter-annotator agreement** (Kappa, F1)
10. **Resolve disagreements** through consensus

### Integration Phase (Week 5)

11. **Merge annotations** with existing data
12. **Update splits** if needed
13. **Run quality checks** on annotated data
14. **Update DATA_CARD.md** with annotation details
15. **Hand off to Persons 3-4** for model training

---

## ğŸ“ Annotation Guidelines Recommendations

### Toxicity Definition

Consider these categories (customize as needed):

1. **Toxic (1):**
   - Hate speech (targeting identity)
   - Threats of violence
   - Severe insults / profanity
   - Harassment
   - Self-harm promotion

2. **Non-Toxic (0):**
   - Constructive criticism
   - Sarcasm (without harm intent)
   - Strong opinions (without targeting)
   - Mild profanity (not directed)

### Code-Mixed Considerations

- Hindi profanity in Roman script (e.g., "gandu", "chutiya")
- Contextual toxicity (Hindi insults may be playful)
- Transliterate if needed: "bhenchod" is toxic
- Consider cultural context: "yaar" (buddy) is not toxic

### Edge Cases

- **Reclaimed slurs:** Check context and speaker identity
- **Quotes:** Is the poster endorsing or denouncing?
- **Sarcasm:** Look for obvious indicators (/s, LOL, emoji)
- **Political discourse:** Strong â‰  toxic; focus on personal attacks

---

## ğŸ”§ Tools & Utilities Available

### Normalization (utils/text_normalization.py)

```python
# Presets
get_normalizer('default')     # Standard cleaning
get_normalizer('strict')      # Aggressive (lowercase, no emoji)
get_normalizer('code_mixed')  # Optimized for Hinglish
get_normalizer('minimal')     # Light touch

# Functions
normalize_text(text, **options)
normalize_romanized_hindi(text)
remove_urls(text)
normalize_punctuation(text)
```

### Deduplication (utils/deduplication.py)

```python
# Remove exact duplicates
df_clean, n_removed = remove_exact_duplicates(df)

# Remove near-duplicates
df_clean, n_removed = remove_near_duplicates(df, threshold=0.95)

# Full pipeline
df_clean, stats = deduplicate_dataframe(
    df, 
    exact=True, 
    near=True,
    near_threshold=0.95
)
```

---

## ğŸ“ˆ Expected Annotation Effort

### Time Estimates (Conservative)

| Task | Samples | Time/Sample | Total Time |
|------|---------|-------------|------------|
| Code-mixed (priority) | 1,869 | 30 sec | 15.6 hours |
| Reddit (remaining) | 8,131 | 20 sec | 45.2 hours |
| YouTube (remaining) | 10,072 | 20 sec | 55.9 hours |
| **TOTAL** | **20,072** | **~21 sec** | **~117 hours** |

### With 3 Annotators

- **Per annotator:** ~39 hours (1 week full-time)
- **With overlap for IAA:** Add 20% = ~47 hours/person
- **Total timeline:** 1-2 weeks for annotation + 1 week for consensus

---

## âœ… Quality Checklist for Person 2

Before handing off to Persons 3-4, ensure:

- [ ] All 20,072 samples annotated
- [ ] Inter-annotator agreement â‰¥0.7 (Kappa)
- [ ] Annotation guidelines documented
- [ ] Edge cases and disagreements resolved
- [ ] Final dataset has same schema as labeled data
- [ ] Data card updated with annotation details
- [ ] Quality report generated
- [ ] Train/dev/test splits updated (if needed)
- [ ] DVC pipeline updated

---

## ğŸš¨ Important Notes

### DO:
- âœ… Use normalization utilities before annotation (cleaner text)
- âœ… Focus on code-mixed samples first (highest value)
- âœ… Track disagreements for guideline improvement
- âœ… Use metadata (subreddit, video_title) for context
- âœ… Update DATA_CARD.md with your work

### DON'T:
- âŒ Change schema (keep columns consistent)
- âŒ Delete any existing fields
- âŒ Expose PII (emails, names already removed)
- âŒ Share raw data publicly (TOS compliance)
- âŒ Skip quality validation before handoff

---

## ğŸ“ Contact & Support

**Person 1 Lead:** [Add your contact]  
**Team Channel:** [Add Slack/Discord]  
**Issues:** [GitHub Issues URL]  
**Documentation:** See `docs/` folder

---

## ğŸ“ Additional Resources

1. **DATA_CARD.md** - Complete dataset documentation (must-read!)
2. **PREPROCESSING_GUIDE.md** - How preprocessing was done
3. **TOS_COMPLIANCE.md** - Legal compliance details
4. **notebooks/collection_walkthrough.ipynb** - Interactive examples

---

## ğŸ Success Criteria for Person 2

You're done when:

1. âœ… All 20,072 samples have labels (0 or 1)
2. âœ… Inter-annotator agreement documented (â‰¥0.7 Kappa)
3. âœ… Final dataset passes quality validation
4. âœ… DATA_CARD.md updated with annotation section
5. âœ… Persons 3-4 can load and use the data without issues

---

**Status:** Ready for annotation! ğŸš€  
**Last Updated:** October 30, 2025  
**Next Steps:** Person 2 begins annotation phase

---

## ğŸ“¦ Appendix: File Manifest

### Critical Files (Must Include in GitHub)

```
âœ… README.md                    - Quick start
âœ… DATA_CARD.md                  - Complete documentation (5,000+ words)
âœ… PERSON1_HANDOFF.md            - This document
âœ… requirements.txt              - All dependencies
âœ… .gitignore                    - Don't commit .env, data/

âœ… data/splits/                  - Train/dev/test (198,603 samples)
âœ… data/unlabeled/               - For annotation (20,072 samples)
âœ… data/reports/                 - Quality & stats

âœ… scripts/*.py                  - All 7 collection/preprocessing scripts
âœ… utils/*.py                    - Normalization & deduplication
âœ… notebooks/*.ipynb             - Collection walkthrough

âœ… .dvc/                         - DVC configuration
âœ… .dvc/.gitignore               - DVC ignore rules
```

### Excluded from GitHub (Too Large)

```
âŒ data/labeled/*.csv            - Too large (80 MB)
   â†’ Use DVC to track
   â†’ Or use Git LFS
   â†’ Or host on HuggingFace/Kaggle

âŒ Input/                        - Raw data (not needed by team)
âŒ Final_input/                  - Duplicate of data/ (already organized)
âŒ .env                          - Credentials (never commit!)
```

---

**Person 1 Sign-Off:** âœ… Complete  
**Ready for Person 2:** âœ… Yes  
**Date:** October 30, 2025
