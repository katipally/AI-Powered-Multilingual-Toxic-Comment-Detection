# Code-Mixed Toxic Comment Detection - Person 1 Deliverables

**Version:** 1.0.0  
**Status:** âœ… **COMPLETE** - Ready for Team  
**Person 1 Lead:** Data Collection & Preprocessing  
**Date:** October 30, 2025  

---

## ğŸ¯ What's Included

This repository contains **all Person 1 deliverables** for the Code-Mixed Toxic Comment Detection project:

- âœ… **218,675 samples** collected & preprocessed
- âœ… **Train/dev/test splits** (70/15/15, stratified)
- âœ… **20,072 unlabeled samples** for Person 2 annotation  
- âœ… **Normalization utilities** (Romanized Hindi, URL/emoji handling)
- âœ… **Deduplication tools** (exact + near-duplicate)
- âœ… **Complete data card** with TOS compliance
- âœ… **Quality reports** (100% validation passed)
- âœ… **Reproducible pipeline** (DVC-ready)

---

## ğŸ“ Repository Structure

```
toxic-comment-detection/
â”œâ”€â”€ README.md                    â† YOU ARE HERE
â”œâ”€â”€ PERSON1_HANDOFF.md           â† ğŸ”¥ START HERE FOR PERSON 2
â”œâ”€â”€ DATA_CARD.md                 â† Complete documentation (must-read!)
â”œâ”€â”€ requirements.txt             â† All dependencies
â”œâ”€â”€ .gitignore                   â† Git ignore rules
â”‚
â”œâ”€â”€ data/                        â† All datasets
â”‚   â”œâ”€â”€ splits/                  â† Train/dev/test (ready for training)
â”‚   â”‚   â”œâ”€â”€ train.csv           (139,022 samples)
â”‚   â”‚   â”œâ”€â”€ dev.csv             (29,790 samples)
â”‚   â”‚   â”œâ”€â”€ test.csv            (29,791 samples)
â”‚   â”‚   â””â”€â”€ split_manifest.json
â”‚   â”‚
â”‚   â”œâ”€â”€ unlabeled/               â† FOR PERSON 2 ANNOTATION â­
â”‚   â”‚   â”œâ”€â”€ for_annotation.csv  (20,072 samples)
â”‚   â”‚   â”œâ”€â”€ reddit.csv
â”‚   â”‚   â””â”€â”€ youtube.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ reports/                 â† Quality & statistics
â”‚   â”‚   â”œâ”€â”€ processing_report.json
â”‚   â”‚   â””â”€â”€ validation_report.json
â”‚   â”‚
â”‚   â””â”€â”€ all_labeled_data.csv     â† Combined labeled data (198,603)
â”‚
â”œâ”€â”€ scripts/                     â† Collection & preprocessing
â”‚   â”œâ”€â”€ 1_download_hatexplain.py
â”‚   â”œâ”€â”€ 2_collect_reddit.py
â”‚   â”œâ”€â”€ 3_collect_youtube.py
â”‚   â”œâ”€â”€ 4_download_textdetox.py
â”‚   â”œâ”€â”€ 5_preprocess_and_unify.py
â”‚   â”œâ”€â”€ 6_data_quality_checks.py
â”‚   â”œâ”€â”€ 7_create_stratified_splits.py
â”‚   â””â”€â”€ find_youtube_videos.py
â”‚
â”œâ”€â”€ utils/                       â† Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ text_normalization.py   â† Normalization + transliteration
â”‚   â””â”€â”€ deduplication.py         â† Dedup utilities
â”‚
â”œâ”€â”€ notebooks/                   â† Jupyter notebooks
â”‚   â””â”€â”€ (to be added)
â”‚
â””â”€â”€ docs/                        â† Additional documentation
    â””â”€â”€ (to be added)
```

---

## ğŸš€ Quick Start

### 1. Clone & Setup

```bash
# Clone repository
git clone [your-repo-url]
cd toxic-comment-detection

# Install dependencies
pip install -r requirements.txt
```

### 2. Verify Data

```bash
# Check data quality
python scripts/6_data_quality_checks.py

# Should output: âœ… VALIDATION PASSED
```

### 3. Load Data

```python
import pandas as pd

# Load train/dev/test splits
train = pd.read_csv('data/splits/train.csv')
dev = pd.read_csv('data/splits/dev.csv')
test = pd.read_csv('data/splits/test.csv')

print(f"Train: {len(train):,} samples")
print(f"Dev:   {len(dev):,} samples")
print(f"Test:  {len(test):,} samples")

# For Person 2: Load unlabeled data
unlabeled = pd.read_csv('data/unlabeled/for_annotation.csv')
print(f"To annotate: {len(unlabeled):,} samples")
```

---

## ğŸ“Š Dataset Overview

| Component | Samples | Status |
|-----------|---------|--------|
| **Labeled (ready to use)** | 198,603 | âœ… Complete |
| - Train | 139,022 | 70% |
| - Dev | 29,790 | 15% |
| - Test | 29,791 | 15% |
| **Unlabeled (for annotation)** | 20,072 | â³ Person 2 |
| - Reddit | 10,000 | Needs labels |
| - YouTube | 10,072 | Needs labels |
| **Code-Mixed (priority)** | 1,869 | Hindi-English |

### Label Distribution (Labeled Data)

- **Non-Toxic (0):** 143,536 (72.3%)
- **Toxic (1):** 55,067 (27.7%)
- **Balance Ratio:** 0.38 (good for training)

### Languages (15+)

English (62.6%), Hindi (4.7%), Spanish (3.8%), Italian (3.8%), Russian, Ukrainian, German, Amharic, Chinese, Arabic, French, Japanese, Turkish, Hebrew, and more.

---

## ğŸ¯ For Person 2 (Annotation Lead)

### ğŸ‘‰ **START HERE:** [PERSON1_HANDOFF.md](PERSON1_HANDOFF.md)

This document contains:
- Complete task overview
- Step-by-step annotation guide
- Tools & utilities documentation
- Quality checklist
- Success criteria

### Your Task

Annotate **20,072 unlabeled samples** as toxic (1) or non-toxic (0):

```python
# Load data
df = pd.read_csv('data/unlabeled/for_annotation.csv')

# Priority: Code-mixed samples
code_mixed = df[df['code_mixed'] == True]
print(f"Priority samples: {len(code_mixed):,}")  # 1,869 samples
```

### Time Estimate

- **Total:** ~117 hours (with 3 annotators: ~39 hours/person)
- **Code-mixed (priority):** ~16 hours
- **Timeline:** 1-2 weeks annotation + 1 week consensus

---

## ğŸ› ï¸ Tools & Utilities

### Normalization (for clean text before annotation)

```python
from utils.text_normalization import get_normalizer

# Use code-mixed preset
normalizer = get_normalizer('code_mixed')
clean_text = normalizer(raw_text)

# Handles:
# - URL/email removal
# - Punctuation normalization
# - Romanized Hindi (yaarâ†’yaar, bhaiiâ†’bhai)
# - Emoji preservation
```

### Deduplication (if needed)

```python
from utils.deduplication import remove_exact_duplicates

df_clean, n_removed = remove_exact_duplicates(df)
print(f"Removed {n_removed} duplicates")
```

---

## ğŸ“š Documentation

### Must-Read Documents

1. **[PERSON1_HANDOFF.md](PERSON1_HANDOFF.md)** - Complete handoff guide for Person 2
2. **[DATA_CARD.md](DATA_CARD.md)** - Full dataset documentation (TOS, licenses, statistics)

### Key Information

- **Random Seed:** 42 (for reproducibility)
- **Validation:** 100% passed all quality checks
- **TOS Compliance:** âœ… All data collected legally via official APIs
- **Licenses:** Mix of CC0, MIT, Apache 2.0 (see DATA_CARD.md)

---

## âœ… Person 1 Completion Checklist

All requirements from official specification completed:

- [x] **Curated dataset with train/dev/test splits (DVC-tracked)**
  - Stratified by label + language + code-mixed
  - Random seed 42 frozen
  - Manifest with complete metadata

- [x] **Normalization & transliteration utilities**
  - Romanized Hindi normalization (50+ words)
  - URL/email/mention removal
  - Validated â‰¥95% accuracy on gold list

- [x] **Provenance logs, data card**
  - Complete 11-section data card
  - License & TOS compliance
  - Collection queries & sampling documented

- [x] **Quality report**
  - Dedup rates: 1.47% (labeled), 14.04% (unlabeled)
  - Language mix: 15+ languages
  - Class balance: 72.3% / 27.7%
  - PII scrubbed (URLs, emails removed)

---

## ğŸ“ˆ Statistics Summary

| Metric | Value |
|--------|-------|
| Total samples | 218,675 |
| Labeled samples | 198,603 (90.8%) |
| Unlabeled samples | 20,072 (9.2%) |
| Languages | 15+ |
| Code-mixed (Hinglish) | 1,869 (0.9%) |
| Sources | 6 datasets |
| Processing time | ~2 hours |
| Cost | $0 (all free tier) |
| Validation status | âœ… 100% passed |

---
